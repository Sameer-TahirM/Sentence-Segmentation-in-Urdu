# Rule-Based Urdu Sentence Segmentation


## Description

This project tackles the challenge of **Urdu Sentence Segmentation**, a fundamental task in Natural Language Processing. Due to the complexities of the Urdu script, such as space omission and complex sentence structures, standard segmentation techniques often fall short.

This implementation provides a from-scratch, rule-based algorithm designed to identify sentence boundaries in a given Urdu text. The system uses a set of linguistic heuristics, including lists of common sentence-ending words, conjunctions, and verbs, to make intelligent segmentation decisions. The project also features a custom evaluation function to measure performance against a manually segmented ground-truth corpus. While the core logic is custom-built, it leverages the `urduhack` library for initial text normalization and tokenization.

## Methodology

The sentence segmentation process is performed in two main steps:

1.  **Normalization and Tokenization**: The raw input text is first normalized to handle character variations and then tokenized into a list of words using the `urduhack` library. This creates a clean, consistent foundation for the segmentation algorithm.

2.  **Rule-Based Boundary Detection**: The core of the project is the `segmentSentence` function, which iterates through the tokens and identifies sentence boundaries based on a set of linguistic rules:
    - **Ending Words**: A pre-defined list of common Urdu sentence-ending words (e.g., "ہے", "ہیں", "گیا") is used to flag potential sentence boundaries.
    - **Conjunction Check**: To prevent incorrect splits, the algorithm checks if the word following a potential boundary is a conjunction (e.g., "اور", "کہ"). If it is, the sentence is not split.
    - **Heuristic Checks**: Additional rules, such as checking for the presence of verbs before a potential end-word, are used to improve accuracy and handle ambiguous cases.

## Features

- **From-Scratch Algorithm**: The core sentence segmentation logic is implemented in Python without relying on pre-built segmentation models.
- **Rule-Based System**: Uses linguistic heuristics with lists of ending words, conjunctions, and verbs to make segmentation decisions.
- **Text Preprocessing**: Utilizes `urduhack` for robust text normalization and tokenization.
- **Custom Evaluation Metric**: A unique, multi-faceted accuracy score is calculated to provide a comprehensive performance measure.
- **Lightweight Dependencies**: Relies primarily on standard Python libraries and `urduhack`.

## Evaluation

The performance of the segmentation algorithm is measured using a custom, multi-faceted accuracy score that averages three distinct metrics (P1, P2, and P3) calculated against a ground-truth segmented corpus.

1.  **P1 - Boundary Count Accuracy**: Compares the *total number* of sentences generated by the algorithm against the number of sentences in the ground-truth text.
2.  **P2 - End-Word Match Accuracy**: Measures the percentage of generated sentences whose *last word* correctly matches the last word of the corresponding ground-truth sentence.
3.  **P3 - Content Similarity Score**: Uses Python's `difflib.SequenceMatcher` to calculate the character-level similarity ratio between each generated sentence and its corresponding ground-truth sentence.

The final accuracy is the arithmetic mean of these three scores, providing a holistic view of the algorithm's performance.

```python
# Final Accuracy Calculation
P = (P1 + P2 + P3) / 3
